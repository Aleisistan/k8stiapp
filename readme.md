# ğŸš€ Infraestructura Full Stack en Kubernetes (Local) con GitOps

Este repositorio contiene la configuraciÃ³n de infraestructura para desplegar una aplicaciÃ³n Full Stack (NestJS + Frontend + Postgres) en un entorno local utilizando **Docker Desktop** y **Kubernetes**, orquestado mediante **Argo CD** y con capacidad de autoescalado (**HPA**).

![Kubernetes](https://img.shields.io/badge/OrquestaciÃ³n-Kubernetes-blue)
![ArgoCD](https://img.shields.io/badge/GitOps-Argo%20CD-orange)
![Postgres](https://img.shields.io/badge/Database-PostgreSQL-336791)
![Status](https://img.shields.io/badge/Status-WIP-yellow)

---

## âœ… Estado Actual del Proyecto (Known Issues)

El despliegue de infraestructura es exitoso, pero existen limitaciones funcionales pendientes de resoluciÃ³n:

* âœ… **Despliegue:** Todos los servicios (Frontend, Backend, DB, Adminer) inician en estado `Running`.
* âœ… **Red Interna:** El Backend resuelve correctamente el DNS `postgres-service`.
* âœ… **Persistencia:** La base de datos mantiene los datos tras reinicios (PVC Configurado).
* âœ… **Autoescalado:** El HPA escala los pods correctamente bajo estrÃ©s.
* âš ï¸ **Funcionalidad de Registro:** âœ… "solved"
    * **Problema:** No es posible crear usuarios desde el Frontend actualmente.âœ… "solved"
    * **DiagnÃ³stico:** Aunque hay conexiÃ³n a la DB, la operaciÃ³n de escritura falla (posible desincronizaciÃ³n de esquemas TypeORM o bloqueo de credenciales CORS).âœ… "solved"
    * **Workaround:** Se pueden verificar conexiones creando tablas manualmente desde Adminer.âœ… "solved" 
### POST https://localhost:3000/users net::ERR_SSL_PROTOCOL_ERROR

El problema era una sola letra: la "s".

EstÃ¡s intentando conectar por HTTPS (Seguro), pero tu Backend en local (NestJS) estÃ¡ corriendo en HTTP (Normal). 
Es como intentar saludar de mano a alguien que te estÃ¡ dando un abrazo; el protocolo no coincide y la conexiÃ³n se rompe antes de empezar.
---

## â˜€ï¸ Ciclo de Vida Diario (Start / Stop)

Instrucciones para detener el trabajo y retomarlo al dÃ­a siguiente sin perder configuraciÃ³n.

### ğŸ›‘ Al terminar el dÃ­a (Stop)
1.  Cierra la aplicaciÃ³n **Docker Desktop** (Click derecho en el icono de la barra de tareas -> *Quit Docker Desktop*).
2.  Apaga tu PC.
    * *Nota:* No borres los recursos con `kubectl delete`. Kubernetes guardarÃ¡ el estado de los pods y el disco de la base de datos.

### â–¶ï¸ Al iniciar el dÃ­a (Start)
1.  Abre **Docker Desktop** y espera a que el icono de Kubernetes estÃ© en verde.
2.  Verifica que los pods revivieron automÃ¡ticamente:
    ```powershell
    kubectl get pods
    ```
3.  **Â¡IMPORTANTE!** Los tÃºneles de acceso se cierran al apagar la PC. **Debes ejecutar estos comandos cada maÃ±ana** para acceder a las herramientas:

    * **Para Argo CD:**
        ```powershell
        kubectl port-forward svc/argocd-server -n argocd 8081:443
        ```
    * **Para Adminer (Base de Datos):**
        ```powershell
        kubectl port-forward svc/adminer-service 8080:8080
        ```

---

## ğŸ› ï¸ Prerrequisitos

* **Docker Desktop** (Kubernetes habilitado en Settings).
* **PowerShell** (Terminal recomendada).
* **Git**.

---

## ğŸš€ GuÃ­a de InstalaciÃ³n desde Cero

### 1. ConstrucciÃ³n de ImÃ¡genes Locales
Construimos las imÃ¡genes directamente en el registro de Docker Desktop para evitar subirlas a la nube.

```powershell
# Backend
cd backend
docker build -t mi-backend:v1 .

# Frontend
cd ../frontend
docker build -t mi-frontend:v1 .
```
---

### 2. ConfiguraciÃ³n del Servidor de MÃ©tricas
Docker Desktop no incluye mÃ©tricas por defecto. Necesarias para que el Autoescalado (HPA) funcione. 

#### 1. Instalar componentes oficiales

```powershell
kubectl apply -f [https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml](https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml)

```

#### 2. Aplicar parche de seguridad para Docker Desktop (Permite certificados inseguros locales)
```powershell
kubectl patch -n kube-system deployment metrics-server --type=json -p "[{\"op\":\"add\",\"path\":\"/spec/template/spec/containers/0/args/-\",\"value\":\"--kubelet-insecure-tls\"}]"
```
---

## 3. InstalaciÃ³n de Argo CD (GitOps)
Instalamos el controlador de Argo CD para que vigile nuestro repositorio.

### Crear namespace e instalar

```powershell
kubectl create namespace argocd
kubectl apply -n argocd -f [https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml](https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml)
```

#### Esperar a que los pods inicien y abrir tÃºnel de acceso (Puerto 8081) el 8080 esta ocupado con adminer

```powershell
kubectl port-forward svc/argocd-server -n argocd 8081:443
```
#### Credenciales de acceso:

URL: https://localhost:8081

Usuario: admin

ContraseÃ±a: Ejecutar el siguiente comando para desencriptarla:

```powershell
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | % { [System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String($_)) }
```

### 4. Configurar la AplicaciÃ³n en Argo CD
Ir a + NEW APP.

Source: Poner la URL de este repositorio github y path k8s.

Destination: Cluster https://kubernetes.default.svc y namespace default.

Sync Policy: Automatic (Self Heal + Prune).


### ğŸ”¥ Pruebas de EstrÃ©s y Autoescalado (HPA)

Para verificar que el sistema escala de 1 a 10 pods, incluimos un archivo de generaciÃ³n de carga.

#### 1. Activar MonitorizaciÃ³n:
Abre una terminal nueva y dÃ©jala corriendo para ver los cambios en vivo:
```powershell
kubectl get hpa -w
```
#### 2. Iniciar el Ataque: Desplegamos el pod generador de carga usando el archivo incluido en el repositorio:

```powershell
kubectl apply -f k8s/load-generator.yaml
```
#### 3. Resultado: En aproximadamente 60 segundos, verÃ¡s en Argo CD o en la terminal cÃ³mo las rÃ©plicas suben progresivamente hasta llegar a 10 pods.

#### 4. Detener prueba: Para detener el ataque, simplemente borra el pod:

```powershell
kubectl delete -f k8s/load-generator.yaml
```
---
## Error de KUBECTL al querer ver los logs del backend
```powershell
kubectl logs -l app=backend -f 
error: you are attempting to follow 10 log streams, but maximum allowed concurrency is 5, use --max-log-requests to increase the limit
```
### ğŸ˜‚ El monstruo que creaste sigue vivo!
Ese error aparece porque tu prueba de estrÃ©s funcionÃ³ demasiado bien. El Autoescalado (HPA) subiÃ³ tu backend a 10 rÃ©plicas, y ahora kubectl te dice: "Oye, no puedo vigilar 10 canales de televisiÃ³n al mismo tiempo, el lÃ­mite es 5".
Intentar depurar un error buscando en 10 logs diferentes es una locura. Vamos a volver a la calma (escalar a 1 solo pod) para que sea fÃ¡cil encontrar el error.

### Argo CD intenta mantener la sincronizaciÃ³n con Git. Si borras el HPA en la interfaz, pero tienes activado el "Auto-Sync", Â¡Argo CD lo volverÃ¡ a crear en 2 segundos!

AquÃ­ te explico cÃ³mo hacerlo correctamente para "pausar" el autoescalado y quedarte con 1 solo pod para depurar:

#### Paso 1: Desactivar el Auto-Sync (Pausar el "piloto automÃ¡tico") â¸ï¸
Si no haces esto, Argo pelearÃ¡ contigo.

Entra a tu aplicaciÃ³n en Argo CD.

Arriba en la cabecera, busca el botÃ³n APP DETAILS.

En la secciÃ³n SYNC POLICY, si dice "Enable Auto-Sync", dale al botÃ³n DISABLE (o quita el check).

Ahora Argo CD dejarÃ¡ de corregir tus cambios manuales.

#### Paso 2: Borrar el HPA desde la Interfaz ğŸ—‘ï¸
En el mapa visual (Tree o Network), busca el cuadradito o hexÃ¡gono que dice hpa backend-hpa.

Haz clic sobre Ã©l.

Dale al botÃ³n DELETE (o a los 3 puntitos -> Delete).

Escribe el nombre para confirmar o dale OK.

Ahora el "jefe" del autoescalado se ha ido.

#### Paso 3: Bajar las rÃ©plicas manualmente ğŸ“‰
Ahora que no hay HPA ni Auto-Sync, tÃº mandas.

Busca el recuadro del Deployment backend.

Haz clic sobre Ã©l.

Busca la opciÃ³n Edit (o a veces hay una pestaÃ±a directa que dice "Scale" o en los 3 puntitos).

Cambia replicas: 10 por replicas: 1.

Dale a Save.

Â¡Listo! VerÃ¡s en pantalla cÃ³mo los pods se ponen en rojo (terminating) y desaparecen hasta quedar solo 1.

#### Â¿CÃ³mo volver a la normalidad?
Cuando termines de depurar:

Ve a APP DETAILS y activa de nuevo el Auto-Sync.

Argo CD verÃ¡ que en Git existe el HPA y que faltan rÃ©plicas.

AutomÃ¡ticamente crearÃ¡ el HPA y dejarÃ¡ todo como estaba.


---
## âš ï¸ Si el archivo load-generator.yaml estÃ¡ dentro de la carpeta k8s/ y haces git push, Argo CD lo interpretarÃ¡ como parte de tu aplicaciÃ³n oficial.

### Argo CD detectarÃ¡ el archivo: VerÃ¡ que hay un nuevo recurso llamado Pod/load-generator.
Lo ejecutarÃ¡ eternamente: Argo CD tiene la misiÃ³n de mantener el estado deseado. Si el generador se detiene, Argo CD podrÃ­a intentar reiniciarlo (aunque tenga restartPolicy: Never, Argo verÃ¡ que el pod "Completed" ensucia el estado y podrÃ­a marcarlo como OutOfSync o tratar de recrearlo si cambias algo).

### Ataque Infinito: Tu backend estarÃ¡ bajo ataque las 24 horas del dÃ­a.

Escalado Permanente: Tu HPA mantendrÃ¡ las 10 rÃ©plicas encendidas siempre, consumiendo toda la CPU de tu mÃ¡quina innecesariamente.

### âŒ Lo que NO debes hacer
No metas el load-generator.yaml dentro de la carpeta k8s/ si esa es la carpeta que vigila Argo CD.

### âœ… La Mejor PrÃ¡ctica (CÃ³mo organizarlo)
Debes separar lo que es Infraestructura Real de lo que son Herramientas de Prueba.

Mueve el archivo a una carpeta separada que Argo CD ignore.

Tu estructura de carpetas recomendada:

Plaintext
mi-proyecto/
â”‚
â”œâ”€â”€ backend/
â”œâ”€â”€ frontend/
â”‚
â”œâ”€â”€ k8s/               <-- Argo CD vigila SOLO esta carpeta
â”‚   â”œâ”€â”€ full_stack.yaml
â”‚   â”œâ”€â”€ backend-hpa.yaml
â”‚   â””â”€â”€ (AQUÃ NO PONGAS EL GENERADOR)
â”‚
â””â”€â”€ tests/             <-- Crea esta carpeta nueva
    â””â”€â”€ load-generator.yaml

### Cuando tÃº quieras hacer la prueba manual, ejecutas el comando desde tu terminal apuntando a esa carpeta:

```PowerShell
# Solo cuando tÃº quieras atacar:
kubectl apply -f tests/load-generator.yaml

# Cuando quieras parar:
kubectl delete -f tests/load-generator.yaml

```
Resumen: Argo CD es para lo que debe estar siempre vivo. Los tests de carga son temporales, asÃ­ que ejecÃºtalos a mano desde una carpeta aparte (tests/ o scripts/).

### ğŸ’¾ Acceso a Base de Datos y Persistencia
El proyecto incluye un volumen persistente (PVC). Los datos sobreviven a reinicios del clÃºster.

Acceso GUI: http://localhost:8080 (Adminer).

Servidor: postgres-service (Â¡Importante: usar nombre interno, no localhost!).

Usuario: postgres

ContraseÃ±a: secret123! (o ver archivo full_stack.yaml).

Frontend: http://localhost:4200

Argo CD: https://localhost:8081

Backend API	http://localhost:3000
